import os
import json
import re
import requests
from datetime import datetime
from langchain_ollama.llms import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def get_ollama_models(ollama_base_url="http://localhost:11434"):
    try:
        response = requests.get(f"{ollama_base_url}/api/tags")
        response.raise_for_status()
        models_data = response.json().get("models", [])
        return [model["name"] for model in models_data]
    except requests.exceptions.ConnectionError:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ ({ollama_base_url})ã€‚è«‹ç¢ºèª Ollama æ­£åœ¨é‹è¡Œã€‚")
        return []
    except Exception as e:
        print(f"âŒ ç²å– Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

class ConversationalRAG:
    def __init__(self, persist_directory, embedding_model_name, llm_model, ollama_base_url,
                 use_history=True, history_summary_threshold=2000):
        self.persist_directory = persist_directory
        self.use_history = use_history
        self.ollama_base_url = ollama_base_url
        self.history_summary_threshold = history_summary_threshold

        print("æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name, model_kwargs={'device': 'cpu'})

        print("æ­£åœ¨åˆå§‹åŒ–/è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        if not os.path.exists(self.persist_directory):
            print("æ‰¾ä¸åˆ°ç¾æœ‰è³‡æ–™åº«ï¼Œå°‡å‰µå»ºä¸€å€‹æ–°çš„ã€‚")
            dummy_doc = Document(page_content="start", metadata={
                                 "source": "initialization", "user_id": "system"})
            self.vector_db = Chroma.from_documents(
                [dummy_doc], self.embeddings, persist_directory=self.persist_directory)
        else:
            print("æ‰¾åˆ°ç¾æœ‰è³‡æ–™åº«ï¼Œæ­£åœ¨è¼‰å…¥...")
            self.vector_db = Chroma(
                persist_directory=self.persist_directory, embedding_function=self.embeddings)

        self.llm = None
        self.current_llm_model = None
        self.set_llm_model(llm_model)

        self.main_prompt = PromptTemplate(
            template='''ä½ æ˜¯ä¸€å€‹ AI åŠ©ç†ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„è³‡æ–™ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

1.  **[ç›¸é—œæ­·å²å°è©±]**: ç”¨å®ƒä¾†ç†è§£å•é¡Œçš„ä¸Šä¸‹æ–‡ï¼Œç¶­æŒå°è©±çš„é€£è²«æ€§ã€‚

å¦‚æœæä¾›çš„è³‡æ–™éƒ½ç„¡æ³•å›ç­”ï¼Œè«‹å‘ŠçŸ¥ä½¿ç”¨è€…ä½ æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚
åœ¨ä½ çš„æœ€çµ‚ç­”æ¡ˆå‰ï¼Œä½ å¯ä»¥ä½¿ç”¨ <think>...</think> æ¨™ç±¤ä¾†å¯«ä¸‹ä½ çš„æ€è€ƒéç¨‹ï¼Œé€™éƒ¨åˆ†å°‡æœƒè¢«å‰ç«¯ä»‹é¢è‡ªå‹•æ‘ºç–Šã€‚

---
[ç›¸é—œæ­·å²å°è©±]:
{history_context}
---

[ä½¿ç”¨è€…ç•¶å‰å•é¡Œ]: {question}

ä½ çš„å›ç­”:''',
            input_variables=["history_context", "question"]
        )

        self.summarizer_prompt = PromptTemplate(
            template="è«‹å°‡ä»¥ä¸‹æä¾›çš„æ–‡å­—å…§å®¹ç¸½çµæˆä¸€æ®µç°¡æ½”ã€æµæš¢çš„æ‘˜è¦ï¼Œä¿ç•™å…¶æ ¸å¿ƒè³‡è¨Šã€‚æ–‡å­—å…§å®¹å¦‚ä¸‹ï¼š\n\n---\n{text_to_summarize}\n---\n\næ‘˜è¦:",
            input_variables=["text_to_summarize"]
        )

    def _get_retriever_for_user(self, user_id: str):
        return self.vector_db.as_retriever(
            search_kwargs={'k': 3, 'filter': {'user_id': user_id}})

    def set_llm_model(self, model_name: str):
        print(f"\nğŸ”„ æ­£åœ¨åˆ‡æ› LLM æ¨¡å‹è‡³: {model_name}")
        try:
            self.llm = OllamaLLM(
                model=model_name, base_url=self.ollama_base_url)
            self.llm.invoke("Hi", stop=["Hi"])
            self.current_llm_model = model_name
            print(f"âœ… LLM æ¨¡å‹æˆåŠŸåˆ‡æ›ç‚º: {self.current_llm_model}")
            return True
        except Exception as e:
            print(f"âŒ åˆ‡æ› LLM æ¨¡å‹å¤±æ•—: {e}")
            return False

    def set_history_retrieval(self, enabled: bool):
        print(f"ğŸ”„ å°‡æ­·å²å°è©±æª¢ç´¢è¨­å®šç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_history = enabled
        return True

    def add_document(self, file_path: str, user_id: str = "global"):
        print(f"ğŸ“„ æ­£åœ¨ç‚ºä½¿ç”¨è€… '{user_id}' è™•ç†æ–°æ–‡ä»¶: {file_path}")
        loader = UnstructuredFileLoader(file_path)
        docs = loader.load()

        for doc in docs:
            doc.metadata['user_id'] = user_id

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        self.vector_db.add_documents(splits)
        print(
            f"âœ… æ–‡ä»¶ '{os.path.basename(file_path)}' å·²æˆåŠŸç‚ºä½¿ç”¨è€… '{user_id}' åŠ å…¥è³‡æ–™åº«ã€‚")

        if os.path.exists(file_path):
            os.remove(file_path)

    def _summarize_text(self, text: str) -> str:
        print(f"ğŸ“ (å…§éƒ¨) æ­£åœ¨ç¸½çµæ–‡å­—ï¼ŒåŸå§‹é•·åº¦: {len(text)}")
        try:
            prompt_value = self.summarizer_prompt.format(
                text_to_summarize=text)
            summary = self.llm.invoke(prompt_value)
            print(f"âœ… (å…§éƒ¨) ç¸½çµå®Œæˆï¼Œæ–°é•·åº¦: {len(summary)}")
            return summary
        except Exception as e:
            print(f"âŒ (å…§éƒ¨) ç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return text[:self.history_summary_threshold]

    def ask(self, question: str, user_id: str, stream: bool = False):
        print(f"\nğŸ¤” æ”¶åˆ°ä¾†è‡ªä½¿ç”¨è€… '{user_id}' çš„è«‹æ±‚ï¼Œå•é¡Œ: '{question}' (æµå¼: {stream})")

        history_context = "æ­·å²å°è©±æª¢ç´¢å·²åœç”¨"
        retrieved_docs = []
        if self.use_history:
            print(f"ğŸ” (å…§éƒ¨) æ­£åœ¨ç‚ºä½¿ç”¨è€… {user_id} æª¢ç´¢æ­·å²å°è©±...")
            user_retriever = self._get_retriever_for_user(user_id)
            retrieved_docs = user_retriever.get_relevant_documents(question)

            if not retrieved_docs:
                history_context = "ç„¡ç›¸é—œæ­·å²å°è©±"
            else:
                context_from_docs = "\n---\n".join(
                    [doc.page_content for doc in retrieved_docs])
                if len(context_from_docs) > self.history_summary_threshold:
                    print(
                        f"â“˜ (å…§éƒ¨) æ­·å²å°è©±éé•· ({len(context_from_docs)} å­—å…ƒ)ï¼Œæ­£åœ¨é€²è¡Œç¸½çµ...")
                    history_context = self._summarize_text(context_from_docs)
                else:
                    history_context = context_from_docs
        else:
            print("â“˜ (å…§éƒ¨) æ­·å²å°è©±æª¢ç´¢å·²åœç”¨ã€‚")

        print("ğŸ“ æ­£åœ¨çµ„åˆ Prompt...")
        formatted_prompt = self.main_prompt.format(
            history_context=history_context,
            question=question
        )

        if stream:
            return self.stream_and_save(question, formatted_prompt, retrieved_docs, user_id)
        else:
            try:
                full_llm_output = self.llm.invoke(formatted_prompt)
                print(f"   -> LLM åŸå§‹è¼¸å‡º:\n{full_llm_output}")
                think_pattern = r"<think>.*?</think>"
                final_answer = re.sub(
                    think_pattern, "", full_llm_output, flags=re.DOTALL).strip()
                self.save_qa(question, full_llm_output, user_id)
                return final_answer
            except Exception as e:
                error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
                print(f"âŒ åœ¨éä¸²æµç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
                return error_msg

    def stream_and_save(self, question, prompt, source_documents, user_id):
        full_answer = ""
        try:
            if source_documents:
                source_data = [
                    {
                        "page_content": doc.page_content,
                        "metadata": doc.metadata
                    }
                    for doc in source_documents
                ]
                yield f"data: {json.dumps({'type': 'sources', 'data': source_data})}"

            for chunk in self.llm.stream(prompt):
                full_answer += chunk
                response_chunk = {"type": "content",
                                  "content": chunk, "error": None}
                yield f"data: {json.dumps(response_chunk)}"

            print(f"ğŸ’¾ æ­£åœ¨ç‚ºä½¿ç”¨è€… {user_id} å„²å­˜æœ¬æ¬¡å•ç­”...")
            self.save_qa(question, full_answer, user_id)

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            print(f"âŒ åœ¨ä¸²æµç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            response_chunk = {"type": "error", "error": error_msg}
            yield f"data: {json.dumps(response_chunk)}"

        finally:
            yield f"data: [DONE]\n"

    def save_qa(self, question, answer, user_id):
        if not answer or answer.strip() == "":
            print("   -> åµæ¸¬åˆ°ç©ºå›ç­”ï¼Œè·³éå„²å­˜ã€‚")
            return

        qa_pair_content = f"å•é¡Œ: {question}\nå›ç­”: {answer}"
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        metadata = {"source": "conversation",
                    "timestamp": current_time, "user_id": user_id}
        new_doc = Document(page_content=qa_pair_content, metadata=metadata)
        self.vector_db.add_documents([new_doc])
        print(f"   -> ä½¿ç”¨è€… {user_id} çš„å°è©±æ­·å²å„²å­˜å®Œç•¢ï¼")
